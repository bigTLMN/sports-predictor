import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_absolute_error
import joblib

# ==========================================
# 1. åŸºç¤ç‰¹å¾µ (Raw Features)
# ==========================================
RAW_FEATURES = [
    'fieldGoalsPercentage', 'threePointersPercentage', 'freeThrowsPercentage',
    'reboundsTotal', 'assists', 'steals', 'blocks', 'turnovers', 
    'plusMinusPoints', 'pointsInThePaint', 'teamScore' # åŠ å…¥å¹³å‡å¾—åˆ†ï¼Œå°å¤§å°åˆ†å¾ˆé‡è¦
]

def load_and_clean_data():
    print("ğŸ“‚ [V3] æ­£åœ¨è®€å– TeamStatistics.csv ...")
    df = pd.read_csv('data/TeamStatistics.csv', low_memory=False)
    
    # æ™‚é–“èˆ‡ç©ºå€¼è™•ç†
    df['gameDateTimeEst'] = pd.to_datetime(df['gameDateTimeEst'], utc=True)
    df = df.dropna(subset=['win', 'teamScore', 'opponentScore'])
    df['win'] = df['win'].astype(int)
    
    # å»ºç«‹å›æ­¸ç›®æ¨™ (Regression Targets)
    # 1. å‹åˆ†å·® (Margin): æ­£æ•¸ä»£è¡¨è´ï¼Œè² æ•¸ä»£è¡¨è¼¸
    df['margin'] = df['teamScore'] - df['opponentScore']
    # 2. ç¸½å¾—åˆ† (Total): å…©éšŠåŠ ç¸½
    df['total_points'] = df['teamScore'] + df['opponentScore']
    
    df = df.sort_values(['teamId', 'gameDateTimeEst'])
    return df

def feature_engineering_v3(df):
    print("ğŸ”„ [V3] ç‰¹å¾µå·¥ç¨‹ï¼šè¨ˆç®— Diff (è®“åˆ†ç”¨) èˆ‡ Sum (å¤§å°åˆ†ç”¨)...")
    
    # 1. è¨ˆç®—è‡ªèº«æ»¾å‹•å¹³å‡
    df_rolled = df.groupby('teamId')[RAW_FEATURES].apply(
        lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()
    )
    new_cols = [f"rolling_{col}" for col in RAW_FEATURES]
    df_rolled.columns = new_cols
    df = pd.concat([df, df_rolled], axis=1)
    df = df.dropna(subset=new_cols)

    # 2. åˆä½µå°æ‰‹æ•¸æ“š
    df_slim = df[['gameId', 'teamId'] + new_cols]
    df_merged = pd.merge(df, df_slim, on='gameId', suffixes=('', '_opp'))
    df_merged = df_merged[df_merged['teamId'] != df_merged['teamId_opp']]
    
    # 3. ç”¢ç”Ÿç‰¹å¾µé›†
    spread_features = [] # ç”¨æ–¼é æ¸¬å‹è²  & è®“åˆ†
    total_features = []  # ç”¨æ–¼é æ¸¬å¤§å°åˆ†
    
    for col in RAW_FEATURES:
        # A. å·®å€¼ç‰¹å¾µ (Diff): ç”¨æ–¼è®“åˆ† (ä¾‹: æˆ‘æ¯”ä½ æº– -> æˆ‘è´åˆ†)
        diff_col = f"diff_{col}"
        df_merged[diff_col] = df_merged[f"rolling_{col}"] - df_merged[f"rolling_{col}_opp"]
        spread_features.append(diff_col)
        
        # B. ç¸½å’Œç‰¹å¾µ (Sum): ç”¨æ–¼å¤§å°åˆ† (ä¾‹: æˆ‘å¿«ä½ ä¹Ÿå¿« -> ç¸½åˆ†é«˜)
        sum_col = f"sum_{col}"
        df_merged[sum_col] = df_merged[f"rolling_{col}"] + df_merged[f"rolling_{col}_opp"]
        total_features.append(sum_col)
    
    # åŠ å…¥ä¸»å ´å› ç´ 
    df_merged['is_home'] = df_merged['home'].apply(lambda x: 1 if x == True else 0)
    spread_features.append('is_home')
    total_features.append('is_home') # å¤§å°åˆ†æœ‰æ™‚å€™è·Ÿä¸»å®¢å ´ä¹Ÿæœ‰é—œ
    
    return df_merged, spread_features, total_features

def train_models(df, spread_cols, total_cols):
    models = {}
    
    # ==========================================
    # Model 1: å‹è² åˆ†é¡ (Classifier)
    # ==========================================
    print(f"\nğŸ¤– è¨“ç·´æ¨¡å‹ 1: å‹è² é æ¸¬ (Win/Loss)...")
    X = df[spread_cols]
    y = df['win']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    clf = xgb.XGBClassifier(
        n_estimators=200, learning_rate=0.03, max_depth=4,
        objective='binary:logistic', eval_metric='logloss', use_label_encoder=False
    )
    clf.fit(X_train, y_train)
    acc = accuracy_score(y_test, clf.predict(X_test))
    print(f"   ğŸ¯ å‹ç‡æº–ç¢ºåº¦: {acc:.2%}")
    models['win'] = clf
    
    # ==========================================
    # Model 2: è®“åˆ†é æ¸¬ (Spread Regressor)
    # ==========================================
    print(f"\nğŸ¤– è¨“ç·´æ¨¡å‹ 2: è®“åˆ†é æ¸¬ (Spread Margin)...")
    y_spread = df['margin'] # ç›®æ¨™æ˜¯è´å¹¾åˆ†
    X_train, X_test, y_train, y_test = train_test_split(X, y_spread, test_size=0.2, shuffle=False)
    
    reg_spread = xgb.XGBRegressor(
        n_estimators=300, learning_rate=0.03, max_depth=5,
        objective='reg:squarederror' # å›æ­¸å•é¡Œ
    )
    reg_spread.fit(X_train, y_train)
    mae_spread = mean_absolute_error(y_test, reg_spread.predict(X_test))
    print(f"   ğŸ“ å¹³å‡èª¤å·® (MAE): {mae_spread:.2f} åˆ† (è¶Šä½è¶Šå¥½)")
    models['spread'] = reg_spread
    
    # ==========================================
    # Model 3: å¤§å°åˆ†é æ¸¬ (Total Regressor)
    # ==========================================
    print(f"\nğŸ¤– è¨“ç·´æ¨¡å‹ 3: å¤§å°åˆ†é æ¸¬ (Total Points)...")
    X_total = df[total_cols] # æ³¨æ„ï¼šé€™è£¡ç”¨ sum ç‰¹å¾µ
    y_total = df['total_points']
    X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, shuffle=False)
    
    reg_total = xgb.XGBRegressor(
        n_estimators=300, learning_rate=0.03, max_depth=5,
        objective='reg:squarederror'
    )
    reg_total.fit(X_train, y_train)
    mae_total = mean_absolute_error(y_test, reg_total.predict(X_test))
    print(f"   ğŸ“ å¹³å‡èª¤å·® (MAE): {mae_total:.2f} åˆ†")
    models['total'] = reg_total
    
    return models

if __name__ == "__main__":
    try:
        df = load_and_clean_data()
        df_final, spread_feats, total_feats = feature_engineering_v3(df)
        
        models = train_models(df_final, spread_feats, total_feats)
        
        # å­˜æª”ï¼šé€™æ¬¡æœ‰ä¸‰å€‹æ¨¡å‹ + å…©çµ„ç‰¹å¾µ
        joblib.dump(models['win'], 'model_win.pkl')
        joblib.dump(models['spread'], 'model_spread.pkl')
        joblib.dump(models['total'], 'model_total.pkl')
        joblib.dump(spread_feats, 'features_spread.pkl')
        joblib.dump(total_feats, 'features_total.pkl')
        
        print("\nğŸ’¾ æ‰€æœ‰æ¨¡å‹èˆ‡ç‰¹å¾µå·²å„²å­˜å®Œç•¢ï¼")
            
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()