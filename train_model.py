import pandas as pd
import xgboost as xgb
from sklearn.metrics import accuracy_score, mean_absolute_error
from sklearn.ensemble import VotingClassifier, VotingRegressor # ğŸ”¥ æ–°å¢ï¼šé›†æˆå­¸ç¿’æ¨¡çµ„
import joblib
import numpy as np

# ==========================================
# 1. å®šç¾©ç‰¹å¾µæ¬„ä½ (æ”¹ç‚ºå‹•æ…‹ç”Ÿæˆ)
# ==========================================
# åŸºç¤æ•¸æ“š (Raw Stats)
BASE_STATS_COLS = [
    'fieldGoalsPercentage', 'threePointersPercentage', 'freeThrowsPercentage',
    'reboundsTotal', 'assists', 'steals', 'blocks', 'turnovers', 
    'plusMinusPoints', 'pointsInThePaint', 'teamScore', 
    'eFG_Percentage', 'TS_Percentage', 'RestDays'
]

# ğŸ”¥ V2.0 å‡ç´šï¼šå®šç¾©å¤šé‡æ™‚é–“çª—å£
ROLLING_WINDOWS = [5, 10, 30] 

# å‹•æ…‹ç”Ÿæˆè¨“ç·´ç‰¹å¾µåˆ—è¡¨
TRAIN_FEATURES_SPREAD = ['is_home'] 
TRAIN_FEATURES_TOTAL = []

for w in ROLLING_WINDOWS:
    for col in BASE_STATS_COLS:
        TRAIN_FEATURES_SPREAD.append(f'diff_rolling_{w}_{col}')
        TRAIN_FEATURES_TOTAL.append(f'sum_rolling_{w}_{col}')
    
    # ğŸ”¥ ç‰¹åˆ¥åŠ å…¥ï¼šå‹ç‡ (Win Rate) ä½œç‚ºå¯¦åŠ›æŒ‡æ¨™
    TRAIN_FEATURES_SPREAD.append(f'diff_rolling_{w}_win_rate')
    TRAIN_FEATURES_TOTAL.append(f'sum_rolling_{w}_win_rate')

# ==========================================
# ğŸ”¥ V8.0 é»ƒé‡‘åƒæ•¸è¨­å®š (ä¾†è‡ª Optuna 2026/01/29 èª¿å„ªçµæœ)
# ==========================================
# æº–ç¢ºç‡: 64.84%
BEST_PARAMS_WIN = {
    'n_estimators': 509,
    'max_depth': 3,
    'learning_rate': 0.043041813813351315,
    'subsample': 0.9183040925737341,
    'colsample_bytree': 0.8256708824079241,
    'gamma': 2.6374110720253743,
    'reg_alpha': 4.528441834346028,
    'reg_lambda': 5.724831419033642,
    'eval_metric': 'logloss',
    'missing': np.nan,
    'n_jobs': 1 # ğŸ”¥ æ”¹ç‚º 1ï¼Œå› ç‚º Voting æœƒå¹³è¡Œè™•ç†å¤šå€‹æ¨¡å‹ï¼Œé¿å… CPU æ¶ä½”
}

# MAE: 11.38
BEST_PARAMS_SPREAD = {
    'n_estimators': 150,
    'max_depth': 3,
    'learning_rate': 0.06353984448063979,
    'subsample': 0.7960172713384834,
    'colsample_bytree': 0.5336338509404283,
    'gamma': 1.6885593428727688,
    'reg_alpha': 0.823932964710099,
    'reg_lambda': 9.364714111214916,
    'objective': 'reg:squarederror',
    'missing': np.nan,
    'n_jobs': 1 
}

# MAE: 15.10
BEST_PARAMS_TOTAL = {
    'n_estimators': 582,
    'max_depth': 3,
    'learning_rate': 0.01207095656064304,
    'subsample': 0.5202392305925475,
    'colsample_bytree': 0.7328987553300463,
    'gamma': 4.3526803881390705,
    'reg_alpha': 5.14487112629654,
    'reg_lambda': 4.892385537038124,
    'objective': 'reg:squarederror',
    'missing': np.nan,
    'n_jobs': 1
}

def load_and_clean_data():
    print("ğŸ“‚ [V8.0] æ­£åœ¨è®€å– TeamStatistics.csv (å¤šé‡çª—å£ç‰¹å¾µç‰ˆ)...")
    try:
        # 1. è®€å–æ•¸æ“š
        req_cols = [
            'gameId', 'teamId', 'gameDateTimeEst', 'home', 'win', 'teamScore', 'opponentScore',
            'fieldGoalsMade', 'fieldGoalsAttempted', 'threePointersMade', 
            'freeThrowsAttempted',
            'fieldGoalsPercentage', 'threePointersPercentage', 'freeThrowsPercentage',
            'reboundsTotal', 'assists', 'steals', 'blocks', 'turnovers', 
            'plusMinusPoints', 'pointsInThePaint'
        ]
        
        df = pd.read_csv('data/TeamStatistics.csv', usecols=lambda c: c in req_cols, low_memory=False)

        # 2. æ—¥æœŸè™•ç†
        df['gameDateTimeEst'] = df['gameDateTimeEst'].astype(str).str.slice(0, 10)
        df['gameDateTimeEst'] = pd.to_datetime(df['gameDateTimeEst'], utc=True, errors='coerce')
        df = df.dropna(subset=['gameDateTimeEst'])
        
        # Concept Drift ä¿®æ­£
        CUTOFF_YEAR = 2015
        print(f"âœ‚ï¸ [Concept Drift Fix] éæ¿¾æ•¸æ“šï¼šåƒ…ä¿ç•™ {CUTOFF_YEAR} å¹´ä»¥å¾Œçš„ç¾ä»£ç±ƒçƒæ•¸æ“š...")
        df = df[df['gameDateTimeEst'].dt.year >= CUTOFF_YEAR]
        
        # 3. æ’åº (é‡è¦)
        df = df.sort_values(['teamId', 'gameDateTimeEst'])

        # ğŸ”¥ ä¿®æ­£ï¼šç§»é™¤ 'win' ç‚º NaN çš„è³‡æ–™
        if df['win'].isnull().any():
            print(f"   âš ï¸ ç™¼ç¾ {df['win'].isnull().sum()} ç­†ç„¡å‹è² çµæœçš„è³‡æ–™(å¯èƒ½æ˜¯æœªä¾†è³½ç¨‹)ï¼Œå·²ç§»é™¤ã€‚")
            df = df.dropna(subset=['win'])

        # 4. ç‰¹å¾µå·¥ç¨‹
        df['threePointersMade'] = df['threePointersMade'].fillna(0)
        df['fieldGoalsAttempted'] = df['fieldGoalsAttempted'].replace(0, np.nan)
        
        df['eFG_Percentage'] = (df['fieldGoalsMade'] + 0.5 * df['threePointersMade']) / df['fieldGoalsAttempted']
        df['TS_Percentage'] = df['teamScore'] / (2 * (df['fieldGoalsAttempted'] + 0.44 * df['freeThrowsAttempted']))
        df['eFG_Percentage'] = df['eFG_Percentage'].fillna(0)
        df['TS_Percentage'] = df['TS_Percentage'].fillna(0)

        df['prev_game_date'] = df.groupby('teamId')['gameDateTimeEst'].shift(1)
        df['RestDays'] = (df['gameDateTimeEst'] - df['prev_game_date']).dt.days
        df['RestDays'] = df['RestDays'].fillna(3).clip(upper=7)
        
        # æ–°å¢ï¼šæ•¸å€¼åŒ–å‹è² 
        df['win_numeric'] = df['win'].astype(int)

        # 5. æ»¾å‹•å¹³å‡
        print("   ğŸ”„ åŸ·è¡Œå¤šé‡æ»¾å‹•å¹³å‡è¨ˆç®— (Windows: 5, 10, 30)...")
        
        cols_to_roll = [c for c in BASE_STATS_COLS if c in df.columns and c != 'RestDays']
        cols_to_roll.append('RestDays')
        
        for w in ROLLING_WINDOWS:
            # 5.1 è¨ˆç®—æ•¸æ“šçµ±è¨ˆå¹³å‡
            rolled_stats = df.groupby('teamId', group_keys=False)[cols_to_roll].apply(
                lambda x: x.shift(1).rolling(w, min_periods=1).mean()
            )
            rolled_stats.columns = [f'rolling_{w}_{c}' for c in rolled_stats.columns]
            
            # 5.2 è¨ˆç®—å‹ç‡ (Win Rate)
            rolled_win = df.groupby('teamId', group_keys=False)['win_numeric'].apply(
                lambda x: x.shift(1).rolling(w, min_periods=1).mean()
            )
            rolled_stats[f'rolling_{w}_win_rate'] = rolled_win
            
            # 5.3 åˆä½µå›ä¸»è¡¨
            df = pd.concat([df, rolled_stats], axis=1)
        
        # 6. æ¸…ç†èˆ‡éæ¿¾
        meta_cols = ['gameId', 'gameDateTimeEst', 'home', 'win', 'teamScore', 'opponentScore']
        keep_cols = meta_cols + [c for c in df.columns if 'rolling_' in c]
        
        df_final = df[keep_cols].rename(columns={
            'teamScore': 'actual_teamScore', 
            'opponentScore': 'actual_opponentScore'
        })
        
        df_final = df_final.dropna(subset=['win', 'actual_teamScore', 'actual_opponentScore'])
        
        print(f"   âœ… è³‡æ–™è™•ç†å®Œæˆï¼ç‰¹å¾µæ•¸å¤§å¹…å¢åŠ ã€‚ç¸½è¡Œæ•¸: {len(df_final)}")
        return df_final

    except Exception as e:
        import traceback
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        exit()

def prepare_training_data(df):
    print(f"ğŸ”„ [V8.0] æº–å‚™å°æˆ°ç‰¹å¾µ...")
    
    df_home = df[df['home'] == 1].copy()
    df_away = df[df['home'] == 0].copy()
    
    merged = pd.merge(df_home, df_away, on='gameId', suffixes=('_h', '_a'))
    merged = merged.sort_values('gameDateTimeEst_h')
    
    merged['is_home'] = 1 
    
    # è‡ªå‹•è¨ˆç®— Diff å’Œ Sum
    needed_features = set()
    for f in TRAIN_FEATURES_SPREAD:
        if f.startswith('diff_'):
            needed_features.add(f.replace('diff_', ''))
            
    for base_col in needed_features:
        h_col = f"{base_col}_h"
        a_col = f"{base_col}_a"
        
        if h_col in merged.columns and a_col in merged.columns:
            merged[f'diff_{base_col}'] = merged[h_col] - merged[a_col]
            merged[f'sum_{base_col}'] = merged[h_col] + merged[a_col]
    
    merged['target_win'] = merged['win_h'] 
    merged['target_margin'] = merged['actual_teamScore_h'] - merged['actual_teamScore_a']
    merged['target_total'] = merged['actual_teamScore_h'] + merged['actual_teamScore_a']
    
    return merged

# ğŸ”¥ æ–°å¢ï¼šå»ºç«‹é›†æˆæ¨¡å‹ (Ensemble Builder)
def create_ensemble_model(base_estimator, params, n_estimators=5, type='classifier'):
    """
    å‰µå»ºä¸€å€‹é›†æˆæ¨¡å‹ï¼ŒåŒ…å« n_estimators å€‹ä¸åŒç¨®å­ç¢¼çš„ XGBoostã€‚
    """
    estimators = []
    print(f"   ğŸ§¬ æ­£åœ¨æ§‹å»ºé›†æˆæ¨¡å‹ (Ensemble size: {n_estimators})...")
    
    for i in range(n_estimators):
        # è¤‡è£½åƒæ•¸ä¸¦è¨­å®šä¸åŒçš„ Random Seed
        model_params = params.copy()
        model_params['random_state'] = 42 + (i * 10) # 42, 52, 62...
        
        model_name = f'xgb_{i}'
        model = base_estimator(**model_params)
        estimators.append((model_name, model))
    
    if type == 'classifier':
        # Soft Voting: å¹³å‡ã€Œæ©Ÿç‡ã€è€Œéå¹³å‡ã€Œçµæœã€ï¼Œé€šå¸¸æ›´æº–ç¢º
        return VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)
    else:
        # Regressor: ç›´æ¥å¹³å‡æ•¸å€¼
        return VotingRegressor(estimators=estimators, n_jobs=-1)

def train():
    df = load_and_clean_data()
    
    if len(df) < 50:
        print(f"âŒ è³‡æ–™é‡éå°‘ï¼Œç„¡æ³•è¨“ç·´ã€‚")
        exit()

    data = prepare_training_data(df)
    
    split_idx = int(len(data) * 0.85)
    train_data = data.iloc[:split_idx]
    test_data = data.iloc[split_idx:]
    
    print(f"\nğŸ“… è¨“ç·´å€é–“: {train_data['gameDateTimeEst_h'].min().date()} ~ {train_data['gameDateTimeEst_h'].max().date()}")
    print(f"ğŸ“… é©—è­‰å€é–“: {test_data['gameDateTimeEst_h'].min().date()} ~ {test_data['gameDateTimeEst_h'].max().date()}")
    
    # ç¢ºä¿åªä½¿ç”¨è³‡æ–™ä¸­å¯¦éš›å­˜åœ¨çš„ç‰¹å¾µ
    available_features_spread = [f for f in TRAIN_FEATURES_SPREAD if f in data.columns]
    available_features_total = [f for f in TRAIN_FEATURES_TOTAL if f in data.columns]
    
    print(f"ğŸš€ ä½¿ç”¨ç‰¹å¾µæ•¸é‡ (Spread): {len(available_features_spread)} (å¼•å…¥å¤šé‡çª—å£)")
    
    # --- æ¨¡å‹ 1: å‹è² é æ¸¬ (Ensemble) ---
    print("\nğŸ¤– è¨“ç·´æ¨¡å‹ 1: å‹è² é æ¸¬ (Win/Loss Ensemble)...")
    # ä½¿ç”¨ VotingClassifier 
    model_win = create_ensemble_model(xgb.XGBClassifier, BEST_PARAMS_WIN, n_estimators=5, type='classifier')
    model_win.fit(train_data[available_features_spread], train_data['target_win'])
    
    acc = accuracy_score(test_data['target_win'], model_win.predict(test_data[available_features_spread]))
    print(f"   ğŸ¯ æœ€çµ‚å›æ¸¬æº–ç¢ºåº¦: {acc*100:.2f}% (Ensemble)")
    
    # --- æ¨¡å‹ 2: è®“åˆ†é æ¸¬ (Ensemble) ---
    print("\nğŸ¤– è¨“ç·´æ¨¡å‹ 2: è®“åˆ†é æ¸¬ (Spread Margin Ensemble)...")
    # ä½¿ç”¨ VotingRegressor
    model_spread = create_ensemble_model(xgb.XGBRegressor, BEST_PARAMS_SPREAD, n_estimators=5, type='regressor')
    model_spread.fit(train_data[available_features_spread], train_data['target_margin'])
    
    mae = mean_absolute_error(test_data['target_margin'], model_spread.predict(test_data[available_features_spread]))
    print(f"   ğŸ“ å¹³å‡èª¤å·® (MAE): {mae:.2f} åˆ† (Ensemble)")
    
    # --- æ¨¡å‹ 3: å¤§å°åˆ†é æ¸¬ (Ensemble) ---
    print("\nğŸ¤– è¨“ç·´æ¨¡å‹ 3: å¤§å°åˆ†é æ¸¬ (Total Points Ensemble)...")
    model_total = create_ensemble_model(xgb.XGBRegressor, BEST_PARAMS_TOTAL, n_estimators=5, type='regressor')
    model_total.fit(train_data[available_features_total], train_data['target_total'])
    
    mae = mean_absolute_error(test_data['target_total'], model_total.predict(test_data[available_features_total]))
    print(f"   ğŸ“ å¹³å‡èª¤å·® (MAE): {mae:.2f} åˆ† (Ensemble)")
    
    # --- å„²å­˜ ---
    # VotingClassifier/Regressor æ˜¯ä¸€å€‹æ¨™æº–çš„ sklearn ç‰©ä»¶ï¼Œå¯ä»¥ç›´æ¥ pickle
    # aggregate_picks.py è¼‰å…¥å¾Œå‘¼å« .predict() è¡Œç‚ºè·Ÿå–®ä¸€æ¨¡å‹ä¸€æ¨¡ä¸€æ¨£
    joblib.dump(model_win, 'model_win.pkl')
    joblib.dump(model_spread, 'model_spread.pkl')
    joblib.dump(model_total, 'model_total.pkl')
    joblib.dump(available_features_spread, 'features_spread.pkl')
    joblib.dump(available_features_total, 'features_total.pkl')
    
    # æ–°å¢ï¼šå„²å­˜çª—å£è¨­å®š
    joblib.dump(ROLLING_WINDOWS, 'rolling_config.pkl') 
    
    print("\nğŸ’¾ V8.0 (Ensemble) æ¨¡å‹è¨“ç·´å®Œæˆï¼æ‰€æœ‰ç³»çµ±å·²å°±ç·’ã€‚")

if __name__ == "__main__":
    train()