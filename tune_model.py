import optuna
import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, mean_absolute_error
# ç¢ºä¿é€™è£¡ import çš„æ˜¯ train_modelï¼Œä¸” train_model.py å…§å®¹å·²ç¶“æ˜¯ V8.0 ç‰ˆæœ¬
from train_model import load_and_clean_data, prepare_training_data, TRAIN_FEATURES_SPREAD, TRAIN_FEATURES_TOTAL

# è®€å–ä¸€æ¬¡è³‡æ–™å³å¯ï¼Œä¸ç”¨é‡è¤‡è®€å–
print("ğŸ“‚ [Tuning] è®€å–è³‡æ–™ä¸­...")
df = load_and_clean_data()
data = prepare_training_data(df)

# ==========================================
# ğŸ”¥ é—œéµä¿®æ­£ï¼šç¢ºä¿ç‰¹å¾µå­˜åœ¨æ–¼ DataFrame ä¸­
# èˆ‡ train_model.py çš„é‚è¼¯ä¿æŒä¸€è‡´ï¼Œé¿å… KeyError
# ==========================================
FEATURES_SPREAD = [f for f in TRAIN_FEATURES_SPREAD if f in data.columns]
FEATURES_TOTAL = [f for f in TRAIN_FEATURES_TOTAL if f in data.columns]

print(f"ğŸš€ [Tuning] å¯¦éš›ä½¿ç”¨ç‰¹å¾µæ•¸é‡: Spread={len(FEATURES_SPREAD)}, Total={len(FEATURES_TOTAL)}")
if len(FEATURES_SPREAD) < 20:
    print("âš ï¸ è­¦å‘Šï¼šç‰¹å¾µæ•¸é‡ä¼¼ä¹ä»æ˜¯èˆŠç‰ˆ (V7.1)ï¼Œè«‹ç¢ºèª train_model.py æ˜¯å¦å·²æ›´æ–°ç‚º V8.0 å¤šé‡çª—å£ç‰ˆæœ¬ã€‚")

# åˆ‡åˆ†è³‡æ–™ (èˆ‡ train_model.py ä¿æŒä¸€è‡´ï¼Œ85% è¨“ç·´ / 15% é©—è­‰)
split_idx = int(len(data) * 0.85)
train_data = data.iloc[:split_idx]
test_data = data.iloc[split_idx:]

print(f"ğŸ“Š è¨“ç·´é›†: {len(train_data)}, é©—è­‰é›†: {len(test_data)}")

# ==========================================
# 1. èª¿å„ªç›®æ¨™ï¼šå‹è² é æ¸¬ (Maximize Accuracy)
# ==========================================
def objective_win(trial):
    # å®šç¾©åƒæ•¸æœå°‹ç©ºé–“
    param = {
        'eval_metric': 'logloss',
        'booster': 'gbtree',
        # ç§»é™¤ 'use_label_encoder': False ä»¥æ¶ˆé™¤è­¦å‘Š
        # é—œéµåƒæ•¸
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'missing': np.nan,
        'n_jobs': -1 # åŠ é€Ÿè¨“ç·´
    }

    model = xgb.XGBClassifier(**param)
    
    # è¨“ç·´ (ä½¿ç”¨éæ¿¾å¾Œçš„ç‰¹å¾µåˆ—è¡¨ FEATURES_SPREAD)
    model.fit(train_data[FEATURES_SPREAD], train_data['target_win'])
    
    # é æ¸¬
    preds = model.predict(test_data[FEATURES_SPREAD])
    accuracy = accuracy_score(test_data['target_win'], preds)
    
    return accuracy

# ==========================================
# 2. èª¿å„ªç›®æ¨™ï¼šè®“åˆ†/å¤§å° (Minimize MAE)
# ==========================================
def objective_reg(trial, target_col, features):
    param = {
        'objective': 'reg:squarederror',
        'booster': 'gbtree',
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'missing': np.nan,
        'n_jobs': -1 # åŠ é€Ÿè¨“ç·´
    }
    
    model = xgb.XGBRegressor(**param)
    # è¨“ç·´
    model.fit(train_data[features], train_data[target_col])
    
    # é æ¸¬
    preds = model.predict(test_data[features])
    mae = mean_absolute_error(test_data[target_col], preds)
    
    return mae

if __name__ == "__main__":
    # è¨­å®š Optuna é¡¯ç¤ºå±¤ç´š
    optuna.logging.set_verbosity(optuna.logging.WARNING)

    print("\nğŸ” é–‹å§‹å°‹æ‰¾ [å‹è² é æ¸¬] çš„é»ƒé‡‘åƒæ•¸...")
    study_win = optuna.create_study(direction='maximize')
    study_win.optimize(objective_win, n_trials=50) 
    print(f"   ğŸ‘‰ Best Accuracy: {study_win.best_value:.4f}")
    
    print("\nğŸ” é–‹å§‹å°‹æ‰¾ [è®“åˆ†é æ¸¬] çš„é»ƒé‡‘åƒæ•¸...")
    study_spread = optuna.create_study(direction='minimize')
    # ä½¿ç”¨éæ¿¾å¾Œçš„ç‰¹å¾µ FEATURES_SPREAD
    study_spread.optimize(lambda trial: objective_reg(trial, 'target_margin', FEATURES_SPREAD), n_trials=50)
    print(f"   ğŸ‘‰ Best MAE: {study_spread.best_value:.4f}")

    print("\nğŸ” é–‹å§‹å°‹æ‰¾ [å¤§å°åˆ†é æ¸¬] çš„é»ƒé‡‘åƒæ•¸...")
    study_total = optuna.create_study(direction='minimize')
    # ä½¿ç”¨éæ¿¾å¾Œçš„ç‰¹å¾µ FEATURES_TOTAL
    study_total.optimize(lambda trial: objective_reg(trial, 'target_total', FEATURES_TOTAL), n_trials=50)
    print(f"   ğŸ‘‰ Best MAE: {study_total.best_value:.4f}")
    
    print("\n" + "="*50)
    print("ğŸ† èª¿å„ªçµæœå ±å‘Š (è«‹å°‡é€™äº›åƒæ•¸å¡«å› train_model.py)")
    print("="*50)
    
    print("\nğŸ¤– [Win Model] Best Accuracy:", study_win.best_value)
    print("Best Params:", study_win.best_params)
    
    print("\nğŸ¤– [Spread Model] Best MAE:", study_spread.best_value)
    print("Best Params:", study_spread.best_params)
    
    print("\nğŸ¤– [Total Model] Best MAE:", study_total.best_value)
    print("Best Params:", study_total.best_params)